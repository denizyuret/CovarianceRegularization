{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "using Knet, AutoGrad, LinearAlgebra, Base.Iterators, Statistics, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "ENV[\"COLUMNS\"] = 64\n",
    "ARRAY=Array{Float64} # KnetArray{Float32}\n",
    "UPDATE=true # keep this true (false only useful for checking gradients)\n",
    "BSIZE=1     # keep batchsize=1 until larger ones supported\n",
    "XSIZE=28*28\n",
    "YSIZE=10\n",
    "HSIZE=[64]\n",
    "ALPHA=100.0\n",
    "GAMMA1=0.0001\n",
    "GAMMA2=0.01\n",
    "LAMBDA=0.995\n",
    "ETA=0.1\n",
    "MU0=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load minibatched MNIST data:\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn, dtst = mnistdata(xtype=ARRAY, batchsize=BSIZE)\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "xtrn = ARRAY(reshape(xtrn,(XSIZE,:)))\n",
    "xtst = ARRAY(reshape(xtst,(XSIZE,:)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition and initialization\n",
    "struct MLP; W; b; μ; B; g1; ∇g1; g2; ∇g2;\n",
    "    function MLP(dims...;α=ALPHA)\n",
    "        h,o = dims[end-1:end]\n",
    "        W = initw.(dims[1:end-1],dims[2:end])\n",
    "        b = initb.(dims[2:end])\n",
    "        μ = initμ(h,o)\n",
    "        B = initB(h,o;α=α)\n",
    "        g1 = initg1(B)\n",
    "        ∇g1 = init∇g1(h)\n",
    "        g2 = initg2(μ)\n",
    "        ∇g2 = init∇g2(h)\n",
    "        new(W, b, μ, B, g1, ∇g1, g2, ∇g2)\n",
    "    end\n",
    "end\n",
    "\n",
    "initw(i,o)=Param(ARRAY(xavier(o,i)))\n",
    "initb(o)=Param(ARRAY(zeros(o,1)))\n",
    "initμ(h,o)=ARRAY(MU0*randn(h,o))\n",
    "initB(h,o;α=ALPHA)=(B = zeros(h,h,o); for i in 1:o, j in 1:h; B[j,j,i] = α; end; ARRAY(B))\n",
    "initg1(B)=[ -sum(logdet.(B[:,:,i] for i in 1:size(B,3))) ]\n",
    "init∇g1(h)=ARRAY(zeros(h,1))\n",
    "initg2(μ)=((d,n)=(0,size(μ,2));for i=1:n-1,j=i+1:n;d-=log(norm(μ[:,i]-μ[:,j])^2);end;[d])\n",
    "init∇g2(h)=ARRAY(zeros(h,1))\n",
    "\n",
    "Base.show(io::IO, m::MLP)=print(IOContext(io,:compact=>true), \"MLP\", (size(m.W[1],2),length.(m.b)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurevec, predict and loss functions\n",
    "function featurevector(m::MLP,x)\n",
    "    L,y = length(m.W),mat(x)\n",
    "    for l in 1:L-1\n",
    "        y = relu.(m.b[l] .+ m.W[l] * y)\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function (m::MLP)(x) # predict\n",
    "    m.b[end] .+ m.W[end] * featurevector(m,x)\n",
    "end\n",
    "\n",
    "function (m::MLP)(x,labels;γ1=GAMMA1,γ2=GAMMA2) # loss\n",
    "    @assert length(labels)==1 \"Batchsize > 1 not implemented yet.\"\n",
    "    yfeat = featurevector(m,x)\n",
    "    ypred = m.b[end] .+ m.W[end] * yfeat\n",
    "    J = nll(ypred,labels)\n",
    "    g1 = sumlogdet(yfeat,labels,m)\n",
    "    g2 = meandist(yfeat,labels,m)\n",
    "    return J + γ1 * g1 + γ2 * g2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes and returns g1 = ∑ logdet(Ci) = -Σ logdet(Bi)\n",
    "# computes m.∇g1 if training()\n",
    "# updates m.g1 and m.B if update=TRUE\n",
    "function sumlogdet(y,labels,m; λ=LAMBDA, η=ETA, update=UPDATE)\n",
    "    β = labels[1]   # β(n) class label for the nth sample\n",
    "    μ = m.μ[:,β:β]  # μ[β(n)](n-1) exponentially weighted mean of class β(n) before the nth sample\n",
    "    B = m.B[:,:,β]  # B[β(n)](n-1) exponentially weighted inverse covariance matrix of class β(n) before the nth sample\n",
    "    \n",
    "    y0 = y - μ      # ybar[L-1](n) the centralized feature vector\n",
    "    z = B * y0      # unscaled gradient\n",
    "    κ = (1-λ)*λ\n",
    "    ξ = 1 / ((1/(1-λ)) + (y0' * B * y0)[1])  # gradient scaling\n",
    "    A = (1/λ)*(B - z*z'*ξ)  \n",
    "    B2 = A-(1-λ)*η*A*A/(1+(1-λ)*η*tr(A))  # updated inverse covariance matrix  \n",
    "    g1 = m.g1[1] + logdet(B) - logdet(B2) # updated -sumlogdet(B)\n",
    "\n",
    "    if training()  # Store gradient if differentiating\n",
    "        m.∇g1 .= 2 * κ * B2 * y0\n",
    "    end\n",
    "    \n",
    "    if update      # Update state if specified\n",
    "        m.g1[1] = g1\n",
    "        m.B[:,:,β] .= B2\n",
    "    end\n",
    "\n",
    "    return g1\n",
    "end\n",
    "\n",
    "@primitive sumlogdet(y,l,m;o...),dy  dy*m.∇g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes and returns g2 = -Σ log |μi-μj|^2\n",
    "# computes m.∇g2 if training()\n",
    "# updates m.g2 and m.μ if update=TRUE\n",
    "function meandist(y,labels,m; λ=LAMBDA, update=UPDATE)\n",
    "    M = size(m.μ,2) # number of classes\n",
    "    β = labels[1]   # β(n) class label for the nth sample\n",
    "    μ1 = m.μ[:,β:β] # μ[β(n)](n-1) exponentially weighted mean of class β(n) before the nth sample\n",
    "    μ2 = λ * μ1 + (1-λ) * y   # updated mean\n",
    "    g2 = 0\n",
    "    if training(); m.∇g2 .= 0; end\n",
    "    for k=1:M\n",
    "        if (k!=β)\n",
    "            olddist = norm(m.μ[:,k:k]-μ1)^2\n",
    "            newdist = norm(m.μ[:,k:k]-μ2)^2\n",
    "            g2 = g2 + log(olddist) - log(newdist)\n",
    "            if training()\n",
    "                m.∇g2 .+= (2 * (1-λ) / newdist) * (m.μ[:,k:k]-μ2)\n",
    "            end\n",
    "        end\n",
    "    end    \n",
    "    if update\n",
    "        m.g2[1] = g2\n",
    "        m.μ[:,β:β] .= μ2\n",
    "    end\n",
    "    return g2\n",
    "end\n",
    "\n",
    "@primitive meandist(y,l,m;o...),dy  dy*m.∇g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x |> summary = \"28×28×1×1 Array{Float64,4}\"\n",
      "labels = UInt8[0x05]\n",
      "(y = featurevector(m, x)) |> summary = \"64×1 Array{Float64,2}\"\n",
      "(scores = m(x)) |> summary = \"10×1 Array{Float64,2}\"\n",
      "J = nll(scores, labels) = 2.4778561096154585\n",
      "g1 = sumlogdet(y, labels, m) = -2945.9880482939625\n",
      "g2 = meandist(y, labels, m) = -35.75083284918922\n",
      "J + GAMMA1 * g1 + GAMMA2 * g2 = -30.557107658243087\n",
      "m(x, labels) = -30.557107658243087\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: check model functions\n",
    "UPDATE=false\n",
    "(x,labels) = first(dtrn)\n",
    "m = MLP(XSIZE,HSIZE...,YSIZE)\n",
    "@show x |> summary\n",
    "@show labels\n",
    "@show (y = featurevector(m,x)) |> summary\n",
    "@show (scores = m(x)) |> summary\n",
    "@show J=nll(scores,labels)\n",
    "@show g1=sumlogdet(y,labels,m)\n",
    "@show g2=meandist(y,labels,m)\n",
    "@show J + GAMMA1 * g1 + GAMMA2 * g2\n",
    "@show m(x,labels)\n",
    "UPDATE=true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#= In[129]:8 =# @gcheck(sumlogdet(py, labels, m)) = true\n",
      "#= In[129]:9 =# @gcheck(meandist(py, labels, m)) = true\n",
      "#= In[129]:10 =# @gcheck(nll(m(x), labels)) = true\n",
      "#= In[129]:11 =# @gcheck(m(x, labels)) = true\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: check gradients\n",
    "using AutoGrad: @gcheck, gcheck\n",
    "(x,labels) = first(dtrn)\n",
    "m = MLP(XSIZE,HSIZE...,YSIZE)\n",
    "y = featurevector(m,x)\n",
    "py = Param(y)\n",
    "UPDATE=false\n",
    "@show @gcheck sumlogdet(py,labels,m)\n",
    "@show @gcheck meandist(py,labels,m)\n",
    "@show @gcheck nll(m(x),labels)\n",
    "@show @gcheck m(x,labels)\n",
    "UPDATE=true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.96e+00  100.00%┣████████┫ 10000/10000 [00:28/00:28, 362.85i/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(acc = 0.919, nll = 0.27138646039786907, g1 = -595.6611794864707, g2 = -235.20288970750443)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 3: train one epoch with regularization\n",
    "Random.seed!(1)\n",
    "m = MLP(XSIZE,HSIZE...,YSIZE)\n",
    "GAMMA1,GAMMA2=0.01,0.1\n",
    "progress!(adam(m,dtst))\n",
    "(acc=accuracy(m,dtst),nll=nll(m(xtst),ytst),g1=initg1(m.B)[1],g2=initg2(m.μ)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56e-05  100.00%┣█████████┫ 10000/10000 [00:29/00:29, 347.50i/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(acc = 0.9198, nll = 0.2600133985319897, g1 = -647.7278184729371, g2 = -232.03334179923695)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 4: train one epoch without regularization\n",
    "Random.seed!(1)\n",
    "m = MLP(XSIZE,HSIZE...,YSIZE)\n",
    "GAMMA1,GAMMA2 = 0,0\n",
    "progress!(adam(m,dtst))\n",
    "(acc=accuracy(m,dtst),nll=nll(m(xtst),ytst),g1=initg1(m.B)[1],g2=initg2(m.μ)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
