{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "using Knet, AutoGrad, LinearAlgebra, Base.Iterators, Statistics, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ARRAY=Array{Float64} # KnetArray{Float32}\n",
    "BSIZE=1\n",
    "XSIZE=28*28\n",
    "YSIZE=10\n",
    "HSIZE=[64]\n",
    "ALPHA=100.0\n",
    "GAMMA=0.0001\n",
    "LAMBDA=0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load minibatched MNIST data:\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn, dtst = mnistdata(xtype=ARRAY, batchsize=BSIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition and initialization\n",
    "struct MLP; W; b; μ; B; g; ∇g;\n",
    "    function MLP(dims...;α=ALPHA)\n",
    "        new(initw.(dims[1:end-1],dims[2:end]),\n",
    "            initb.(dims[2:end]),\n",
    "            initμ(dims[end-1],dims[end]),\n",
    "            initB(dims[end-1],dims[end],α=α),\n",
    "            initg(dims[end-1],dims[end],α=α), \n",
    "            init∇g(dims[end-1],dims[end]))\n",
    "    end\n",
    "end\n",
    "\n",
    "initw(i,o)=Param(ARRAY(xavier(o,i)))\n",
    "initb(o)=Param(ARRAY(zeros(o,1)))\n",
    "initμ(h,o)=ARRAY(zeros(h,o))\n",
    "initB(h,o;α=ALPHA)=(B = zeros(h,h,o); for i in 1:o, j in 1:h; B[j,j,i] = α; end; ARRAY(B))\n",
    "initg(h,o;α=ALPHA)=ARRAY([-h*o*log(α)])\n",
    "init∇g(h,o)=ARRAY(zeros(h,1))\n",
    "\n",
    "Base.show(io::IO, m::MLP)=print(IOContext(io,:compact=>true), \"MLP\", (size(m.W[1],2),length.(m.b)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and loss functions\n",
    "\n",
    "function featurevector(m::MLP,x)\n",
    "    L,y = length(m.W),mat(x)\n",
    "    for l in 1:L-1\n",
    "        y = relu.(m.b[l] .+ m.W[l] * y)\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function (m::MLP)(x) # predict\n",
    "    m.b[end] .+ m.W[end] * featurevector(m,x)\n",
    "end\n",
    "\n",
    "function (m::MLP)(x,labels; γ=GAMMA, update=true) # loss\n",
    "    yfeat = featurevector(m,x)\n",
    "    ypred = m.b[end] .+ m.W[end] * yfeat\n",
    "    J = nll(ypred,labels)  # per instance average negative log likelihood loss\n",
    "    g = sumlogdet(yfeat,labels,m; update=update)\n",
    "    return J + γ * g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization function and its derivative; assume batchsize=1 for now\n",
    "function sumlogdet(y,labels,m; λ=LAMBDA, update=false)\n",
    "    @assert length(labels)==1 \"Batchsize > 1 not implemented yet.\"\n",
    "\n",
    "    λ = convert(eltype(y),λ)\n",
    "    β = labels[1]   # β(n) class label for the nth sample\n",
    "    μ = m.μ[:,β:β]  # μ[β(n)](n-1) exponentially weighted mean of class β(n) before the nth sample\n",
    "    B = m.B[:,:,β]  # B[β(n)](n-1) exponentially weighted inverse covariance matrix of class β(n) before the nth sample\n",
    "    \n",
    "    y0 = y - μ      # ybar[L-1](n) the centralized feature vector\n",
    "    z = B * y0      # unscaled gradient\n",
    "    ξ = 1 / ((1/(1-λ)) + (y0' * B * y0)[1])  # gradient scaling\n",
    "    B2 = (1/λ)*(B - z*z'*ξ)  # updated inverse covariance matrix\n",
    "    g = m.g[1] + logdet(B) - logdet(B2)  # updated -sumlogdet(B)\n",
    "\n",
    "    if training()  # Store gradient if differentiating\n",
    "        m.∇g .= 2 * ξ * z\n",
    "    end\n",
    "    \n",
    "    if update      # Update state if specified\n",
    "        m.g[1] = g\n",
    "        m.B[:,:,β] .= B2\n",
    "        m.μ[:,β:β] .= λ * μ + (1-λ) * y\n",
    "    end\n",
    "\n",
    "    return g\n",
    "end\n",
    "\n",
    "function sumlogdetback(m)\n",
    "    m.∇g\n",
    "end\n",
    "\n",
    "@primitive sumlogdet(y,labels,model;o...),dy  dy*sumlogdetback(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with different hyperparameters (using dtst because it is small)\n",
    "HSIZE=[64]\n",
    "GAMMA=0.0001\n",
    "LAMBDA=0.995\n",
    "ALPHA=100.0\n",
    "Random.seed!(1)\n",
    "m = MLP(XSIZE,HSIZE...,YSIZE)\n",
    "progress!(adam(m,dtst))\n",
    "accuracy(m,dtst)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
